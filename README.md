

# Airflow Sales ETL Pipeline 

An end-to-end **ETL + Data Warehouse** project built with **Apache Airflow, Docker, Python, and SQLite** using the Superstore dataset to transform raw sales data into a structured analytical data warehouse.

This project demonstrates how a real-world data engineering pipeline is designed, orchestrated, monitored, and validated using production-style practices.

---

##  What this project does

The pipeline:

1. Ingests raw sales data from `include/Superstore.csv`
2. Cleans and enriches the data with business metrics
3. Loads it into a **staging table (`sales_data`)**
4. Builds a **star-schema data warehouse**:

   * Dimensions: `dim_customers`, `dim_products`, `dim_dates`, `dim_location`
   * Fact: `fact_sales`
5. Runs **automated data-quality tests** using `pytest`

The main Airflow DAG is:

```
superstore_data_pipeline_sqlite
```

---

##  Architecture Overview

### Data Source & Storage

* **Input:** `include/Superstore.csv`
* **Warehouse:** `include/superstore.db` (SQLite â€” created at runtime by Airflow)

> Note: The SQLite warehouse is generated automatically when the DAG runs inside Docker.

---

##  Star Schema (Warehouse Design)

![Star Schema](images/star_schema.png)

This diagram shows:

* Central fact table: **`fact_sales`**
* Surrounding dimensions:

  * `dim_customers`
  * `dim_products`
  * `dim_dates`
  * `dim_location`

### Why this matters

* Separates **business facts (sales, profit, quantity)** from **descriptive attributes (customer, product, date, location)**
* Enables fast analytics
* Mirrors industry-standard data warehouse design

---

## âš™ï¸ Airflow Orchestration

### DAG Files

* **`dags/superstore_pipeline_sqlite.py`**

  * Main orchestration DAG controlling the entire workflow

* **`dags/superstore_transformation.py`**

  * Contains reusable transformation functions:

    * `create_dim_customers`
    * `create_dim_products`
    * `create_dim_dates`
    * `create_dim_location`
    * `create_fact_sales`

---

##  Pipeline Flow (Task Graph)

![Airflow DAG Graph](images/airflow_graph.png)

### Step-by-step execution

1. **process_and_load_data (Extractâ€“Transformâ€“Load)**

   * Reads CSV
   * Standardizes schema
   * Parses dates
   * Creates metrics:

     * `profit_margin`
     * `discount_amount`
     * `shipping_duration`
     * `profit_category`
     * `sales_tier`
   * Loads into **staging table `sales_data`**

2. **Build Dimensions (in parallel)**

   * `create_dim_customers`
   * `create_dim_products`
   * `create_dim_dates`
   * `create_dim_location`

3. **Build Fact**

   * `create_fact_sales`
   * Joins staging with all dimensions
   * Creates final analytical table **`fact_sales`**

This parallel design improves performance and reflects real production pipelines.

---

##  Airflow Grid View (Successful Run)

![Airflow Grid](images/airflow_grid.png)

The grid view confirms that:

* Staging completed
* All dimension tables were built
* Fact table was created successfully

---

##  SQLite Warehouse Output

![SQLite Tables](images/sqlite_tables.png)

After the DAG runs, your warehouse contains:

```
sales_data  
dim_customers  
dim_products  
dim_dates  
dim_location  
fact_sales   (~9,986 rows)  
```

Inspect inside Docker:

```bash
docker compose exec airflow-webserver sqlite3 /opt/airflow/include/superstore.db
.tables
SELECT COUNT(*) FROM fact_sales;
```

---

## ğŸ› ï¸ Tech Stack

* **Apache Airflow** â€” workflow orchestration
* **Docker** â€” containerized runtime
* **Python & Pandas** â€” data transformation
* **SQLAlchemy + sqlite3** â€” DB access
* **SQLite** â€” lightweight analytical warehouse
* **Pytest** â€” automated data validation

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ superstore_pipeline_sqlite.py
â”‚   â”œâ”€â”€ superstore_transformation.py
â”‚   â””â”€â”€ exampledag.py
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ Superstore.csv
â”‚   â””â”€â”€ superstore.db   # created at runtime
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ sales_data.py
â”‚   â”œâ”€â”€ dim_customers.py
â”‚   â”œâ”€â”€ dim_products.py
â”‚   â”œâ”€â”€ dim_dates.py
â”‚   â”œâ”€â”€ dim_location.py
â”‚   â””â”€â”€ fact_sales.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ dags/test_dag_example.py
â”‚   â””â”€â”€ test_data_quality.py
â”œâ”€â”€ images/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## â–¶ï¸ How to Run (Docker â€” Recommended)

Start Airflow:

```bash
docker compose up
```

Open UI:

```
http://localhost:8080
```

Trigger DAG:

```
superstore_data_pipeline_sqlite
```

---

## ğŸ§ª Data Quality Tests

Tests are in:

```
tests/test_data_quality.py
```

They check:

* `fact_sales` exists and is populated
* No missing critical keys
* No negative sales
* No non-positive quantity
* No duplicate `order_product_id`
* Every sale has a valid customer

Run all tests:

```bash
pytest -q
```

Run only data quality tests:

```bash
pytest -q tests/test_data_quality.py
```

---

## ğŸš€ Future Improvements

* Migrate SQLite â†’ PostgreSQL / Snowflake
* Add incremental loads
* Add dbt layer
* Add Power BI / Tableau dashboard
* Add CI pipeline for automated testing
